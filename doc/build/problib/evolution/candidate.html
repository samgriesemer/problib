<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>problib.evolution.candidate API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>problib.evolution.candidate</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import string

from ..combinatorics import counting
from ..ml import nn

class Candidate:
  &#39;&#39;&#39;
  Base candidate class for evolutionary algorithms
  NOTE: can always consider switching the constructor to
  by default take random attributes and yield a stochastic
  candidate. This might make sense if we are to strictly
  follow what is most commonly used. However, it&#39;s not
  obvious how to go from specified genotype into a constructor
  expecting parameters for random generation; this would certainly
  be more sloppy than the current simple entry point constructor.

  Want to separate candidate from agent. Candidates dont need to be 
  defined in the context of a gym evnironment. They just hold a genotype
  and inherit the basic functions seen in base
  &#39;&#39;&#39;
  def __init__(self, genotype):
    #create(**kwargs)
    self.genotype = genotype

  def __str__(self):
    return self.epigenesis()

  @classmethod
  def random(cls, genotype):
    &#39;&#39;&#39;
    Alternate constructor for random candidate construction,
    to be implemented by subclassing type
    &#39;&#39;&#39;
    pass

  def epigenesis(self):
    &#39;&#39;&#39;Process of turning genotype into phenotype&#39;&#39;&#39;
    return self.genotype

class AlphaString(Candidate):
  &#39;&#39;&#39;Candidate child for genetic string&#39;&#39;&#39;
  @classmethod
  def random(cls, length, alphabet=string.printable):
    &#39;&#39;&#39;
    Create random AlphaString

    alphastr = AlphaString.random(length)
    alphastr = AlphaString.random(length, &#39;abc&#39;)

    :genotype: list (mutable)
    :phenotype: conversion to string
    &#39;&#39;&#39;
    gene = counting.Product(*[alphabet]*length)
    gene = list(next(gene.sample()))
    return cls(gene)

  def epigenesis(self):
    return &#39;&#39;.join(self.genotype)

class BitString(AlphaString):
  &#39;&#39;&#39;Candidate child for genetic string&#39;&#39;&#39;
  @classmethod
  def random(cls, length):
    return super().random(length, &#39;01&#39;)

class NeuralNetwork(Candidate):
  &#39;&#39;&#39;
  NeuralNetwork candidate object for use in
  neuroevolution implementations. This candidate
  has a phenotype represented by its observable
  actions resulting from inference, and a genotype
  represented by its underlying internal network
  structure and weights. All evolution operations (as
  usual) are performed on the genotype level.

  :phenotype: output from inference and resulting behavior
  :genotype: internal network structure and weight values
  &#39;&#39;&#39;
  def __init__(self, genotype):
    &#39;&#39;&#39;
    Genotype expected to be of the form of `.weights`
    attribute from the NeuralNetwork class (i.e. a list
    of NumPy arrays)
    THIS METHOD CURRENTLY NOT NEEDED
    &#39;&#39;&#39;
    self.genotype = genotype
    self.time_alive = 0

  @classmethod
  def random(cls, layers, rng=1):
    &#39;&#39;&#39;
    Take layers structure as input, instantiate neural
    network with given layers, set random weights according
    to [-rng, +rng]

    :layers: list of network layer size
    :rng: weights generated from [-rng, +rng]
    &#39;&#39;&#39;
    net = nn.NeuralNetwork(layers, epsilon=rng)
    return cls(net.weights)

  def epigenesis(self):
    &#39;&#39;&#39;
    Convert from network structure to observable actions
    via inference on live neural network architecture using
    genotype weights. This process requires a data point on
    which to evaluate the network

    TODO: consider how this is being done; should a nn object
    be kept in memory at all times and modifications be made
    directly to its weights so come inference time everything is
    ready to go? This seems a little bulky but may end up being
    more efficient. Initializing a network each time from weights
    though has a tiny overhead; it just sets the nn object&#39;s weights
    and no additional computation is needed.
    Also how are we going to pass the incoming data to the network
    for the actual inference procedure? Should the data be set to
    the network itself, passed to the function, or set under the
    candidate object?
    &#39;&#39;&#39;
    net = nn.NeuralNetwork.from_weights(self.genotype)
    return net

  def predict(self, x):
    net = self.epigenesis()
    data = [x[prop] for prop in [&#39;px&#39;,&#39;py&#39;,&#39;vx&#39;,&#39;vy&#39;,&#39;ax&#39;,&#39;ay&#39;]]
    return net.predict(np.array(data))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="problib.evolution.candidate.AlphaString"><code class="flex name class">
<span>class <span class="ident">AlphaString</span></span>
<span>(</span><span>genotype)</span>
</code></dt>
<dd>
<section class="desc"><p>Candidate child for genetic string</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AlphaString(Candidate):
  &#39;&#39;&#39;Candidate child for genetic string&#39;&#39;&#39;
  @classmethod
  def random(cls, length, alphabet=string.printable):
    &#39;&#39;&#39;
    Create random AlphaString

    alphastr = AlphaString.random(length)
    alphastr = AlphaString.random(length, &#39;abc&#39;)

    :genotype: list (mutable)
    :phenotype: conversion to string
    &#39;&#39;&#39;
    gene = counting.Product(*[alphabet]*length)
    gene = list(next(gene.sample()))
    return cls(gene)

  def epigenesis(self):
    return &#39;&#39;.join(self.genotype)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="problib.evolution.candidate.Candidate" href="#problib.evolution.candidate.Candidate">Candidate</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="problib.evolution.candidate.BitString" href="#problib.evolution.candidate.BitString">BitString</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="problib.evolution.candidate.AlphaString.random"><code class="name flex">
<span>def <span class="ident">random</span></span>(<span>length, alphabet=&#x27;0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!&quot;#$%&amp;\&#x27;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~ \t\n\r\x0b\x0c&#x27;)</span>
</code></dt>
<dd>
<section class="desc"><p>Create random AlphaString</p>
<p>alphastr = AlphaString.random(length)
alphastr = AlphaString.random(length, 'abc')</p>
<p>:genotype: list (mutable)
:phenotype: conversion to string</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def random(cls, length, alphabet=string.printable):
  &#39;&#39;&#39;
  Create random AlphaString

  alphastr = AlphaString.random(length)
  alphastr = AlphaString.random(length, &#39;abc&#39;)

  :genotype: list (mutable)
  :phenotype: conversion to string
  &#39;&#39;&#39;
  gene = counting.Product(*[alphabet]*length)
  gene = list(next(gene.sample()))
  return cls(gene)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="problib.evolution.candidate.Candidate" href="#problib.evolution.candidate.Candidate">Candidate</a></b></code>:
<ul class="hlist">
<li><code><a title="problib.evolution.candidate.Candidate.epigenesis" href="#problib.evolution.candidate.Candidate.epigenesis">epigenesis</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="problib.evolution.candidate.BitString"><code class="flex name class">
<span>class <span class="ident">BitString</span></span>
<span>(</span><span>genotype)</span>
</code></dt>
<dd>
<section class="desc"><p>Candidate child for genetic string</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BitString(AlphaString):
  &#39;&#39;&#39;Candidate child for genetic string&#39;&#39;&#39;
  @classmethod
  def random(cls, length):
    return super().random(length, &#39;01&#39;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="problib.evolution.candidate.AlphaString" href="#problib.evolution.candidate.AlphaString">AlphaString</a></li>
<li><a title="problib.evolution.candidate.Candidate" href="#problib.evolution.candidate.Candidate">Candidate</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="problib.evolution.candidate.AlphaString" href="#problib.evolution.candidate.AlphaString">AlphaString</a></b></code>:
<ul class="hlist">
<li><code><a title="problib.evolution.candidate.AlphaString.epigenesis" href="#problib.evolution.candidate.Candidate.epigenesis">epigenesis</a></code></li>
<li><code><a title="problib.evolution.candidate.AlphaString.random" href="#problib.evolution.candidate.AlphaString.random">random</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="problib.evolution.candidate.Candidate"><code class="flex name class">
<span>class <span class="ident">Candidate</span></span>
<span>(</span><span>genotype)</span>
</code></dt>
<dd>
<section class="desc"><p>Base candidate class for evolutionary algorithms
NOTE: can always consider switching the constructor to
by default take random attributes and yield a stochastic
candidate. This might make sense if we are to strictly
follow what is most commonly used. However, it's not
obvious how to go from specified genotype into a constructor
expecting parameters for random generation; this would certainly
be more sloppy than the current simple entry point constructor.</p>
<p>Want to separate candidate from agent. Candidates dont need to be
defined in the context of a gym evnironment. They just hold a genotype
and inherit the basic functions seen in base</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Candidate:
  &#39;&#39;&#39;
  Base candidate class for evolutionary algorithms
  NOTE: can always consider switching the constructor to
  by default take random attributes and yield a stochastic
  candidate. This might make sense if we are to strictly
  follow what is most commonly used. However, it&#39;s not
  obvious how to go from specified genotype into a constructor
  expecting parameters for random generation; this would certainly
  be more sloppy than the current simple entry point constructor.

  Want to separate candidate from agent. Candidates dont need to be 
  defined in the context of a gym evnironment. They just hold a genotype
  and inherit the basic functions seen in base
  &#39;&#39;&#39;
  def __init__(self, genotype):
    #create(**kwargs)
    self.genotype = genotype

  def __str__(self):
    return self.epigenesis()

  @classmethod
  def random(cls, genotype):
    &#39;&#39;&#39;
    Alternate constructor for random candidate construction,
    to be implemented by subclassing type
    &#39;&#39;&#39;
    pass

  def epigenesis(self):
    &#39;&#39;&#39;Process of turning genotype into phenotype&#39;&#39;&#39;
    return self.genotype</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="problib.evolution.candidate.AlphaString" href="#problib.evolution.candidate.AlphaString">AlphaString</a></li>
<li><a title="problib.evolution.candidate.NeuralNetwork" href="#problib.evolution.candidate.NeuralNetwork">NeuralNetwork</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="problib.evolution.candidate.Candidate.random"><code class="name flex">
<span>def <span class="ident">random</span></span>(<span>genotype)</span>
</code></dt>
<dd>
<section class="desc"><p>Alternate constructor for random candidate construction,
to be implemented by subclassing type</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def random(cls, genotype):
  &#39;&#39;&#39;
  Alternate constructor for random candidate construction,
  to be implemented by subclassing type
  &#39;&#39;&#39;
  pass</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="problib.evolution.candidate.Candidate.epigenesis"><code class="name flex">
<span>def <span class="ident">epigenesis</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Process of turning genotype into phenotype</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def epigenesis(self):
  &#39;&#39;&#39;Process of turning genotype into phenotype&#39;&#39;&#39;
  return self.genotype</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="problib.evolution.candidate.NeuralNetwork"><code class="flex name class">
<span>class <span class="ident">NeuralNetwork</span></span>
<span>(</span><span>genotype)</span>
</code></dt>
<dd>
<section class="desc"><p>NeuralNetwork candidate object for use in
neuroevolution implementations. This candidate
has a phenotype represented by its observable
actions resulting from inference, and a genotype
represented by its underlying internal network
structure and weights. All evolution operations (as
usual) are performed on the genotype level.</p>
<p>:phenotype: output from inference and resulting behavior
:genotype: internal network structure and weight values</p>
<p>Genotype expected to be of the form of <code>.weights</code>
attribute from the NeuralNetwork class (i.e. a list
of NumPy arrays)
THIS METHOD CURRENTLY NOT NEEDED</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NeuralNetwork(Candidate):
  &#39;&#39;&#39;
  NeuralNetwork candidate object for use in
  neuroevolution implementations. This candidate
  has a phenotype represented by its observable
  actions resulting from inference, and a genotype
  represented by its underlying internal network
  structure and weights. All evolution operations (as
  usual) are performed on the genotype level.

  :phenotype: output from inference and resulting behavior
  :genotype: internal network structure and weight values
  &#39;&#39;&#39;
  def __init__(self, genotype):
    &#39;&#39;&#39;
    Genotype expected to be of the form of `.weights`
    attribute from the NeuralNetwork class (i.e. a list
    of NumPy arrays)
    THIS METHOD CURRENTLY NOT NEEDED
    &#39;&#39;&#39;
    self.genotype = genotype
    self.time_alive = 0

  @classmethod
  def random(cls, layers, rng=1):
    &#39;&#39;&#39;
    Take layers structure as input, instantiate neural
    network with given layers, set random weights according
    to [-rng, +rng]

    :layers: list of network layer size
    :rng: weights generated from [-rng, +rng]
    &#39;&#39;&#39;
    net = nn.NeuralNetwork(layers, epsilon=rng)
    return cls(net.weights)

  def epigenesis(self):
    &#39;&#39;&#39;
    Convert from network structure to observable actions
    via inference on live neural network architecture using
    genotype weights. This process requires a data point on
    which to evaluate the network

    TODO: consider how this is being done; should a nn object
    be kept in memory at all times and modifications be made
    directly to its weights so come inference time everything is
    ready to go? This seems a little bulky but may end up being
    more efficient. Initializing a network each time from weights
    though has a tiny overhead; it just sets the nn object&#39;s weights
    and no additional computation is needed.
    Also how are we going to pass the incoming data to the network
    for the actual inference procedure? Should the data be set to
    the network itself, passed to the function, or set under the
    candidate object?
    &#39;&#39;&#39;
    net = nn.NeuralNetwork.from_weights(self.genotype)
    return net

  def predict(self, x):
    net = self.epigenesis()
    data = [x[prop] for prop in [&#39;px&#39;,&#39;py&#39;,&#39;vx&#39;,&#39;vy&#39;,&#39;ax&#39;,&#39;ay&#39;]]
    return net.predict(np.array(data))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="problib.evolution.candidate.Candidate" href="#problib.evolution.candidate.Candidate">Candidate</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="problib.evolution.candidate.NeuralNetwork.random"><code class="name flex">
<span>def <span class="ident">random</span></span>(<span>layers, rng=1)</span>
</code></dt>
<dd>
<section class="desc"><p>Take layers structure as input, instantiate neural
network with given layers, set random weights according
to [-rng, +rng]</p>
<p>:layers: list of network layer size
:rng: weights generated from [-rng, +rng]</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def random(cls, layers, rng=1):
  &#39;&#39;&#39;
  Take layers structure as input, instantiate neural
  network with given layers, set random weights according
  to [-rng, +rng]

  :layers: list of network layer size
  :rng: weights generated from [-rng, +rng]
  &#39;&#39;&#39;
  net = nn.NeuralNetwork(layers, epsilon=rng)
  return cls(net.weights)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="problib.evolution.candidate.NeuralNetwork.epigenesis"><code class="name flex">
<span>def <span class="ident">epigenesis</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert from network structure to observable actions
via inference on live neural network architecture using
genotype weights. This process requires a data point on
which to evaluate the network</p>
<p>TODO: consider how this is being done; should a nn object
be kept in memory at all times and modifications be made
directly to its weights so come inference time everything is
ready to go? This seems a little bulky but may end up being
more efficient. Initializing a network each time from weights
though has a tiny overhead; it just sets the nn object's weights
and no additional computation is needed.
Also how are we going to pass the incoming data to the network
for the actual inference procedure? Should the data be set to
the network itself, passed to the function, or set under the
candidate object?</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def epigenesis(self):
  &#39;&#39;&#39;
  Convert from network structure to observable actions
  via inference on live neural network architecture using
  genotype weights. This process requires a data point on
  which to evaluate the network

  TODO: consider how this is being done; should a nn object
  be kept in memory at all times and modifications be made
  directly to its weights so come inference time everything is
  ready to go? This seems a little bulky but may end up being
  more efficient. Initializing a network each time from weights
  though has a tiny overhead; it just sets the nn object&#39;s weights
  and no additional computation is needed.
  Also how are we going to pass the incoming data to the network
  for the actual inference procedure? Should the data be set to
  the network itself, passed to the function, or set under the
  candidate object?
  &#39;&#39;&#39;
  net = nn.NeuralNetwork.from_weights(self.genotype)
  return net</code></pre>
</details>
</dd>
<dt id="problib.evolution.candidate.NeuralNetwork.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, x):
  net = self.epigenesis()
  data = [x[prop] for prop in [&#39;px&#39;,&#39;py&#39;,&#39;vx&#39;,&#39;vy&#39;,&#39;ax&#39;,&#39;ay&#39;]]
  return net.predict(np.array(data))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="problib.evolution" href="index.html">problib.evolution</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="problib.evolution.candidate.AlphaString" href="#problib.evolution.candidate.AlphaString">AlphaString</a></code></h4>
<ul class="">
<li><code><a title="problib.evolution.candidate.AlphaString.random" href="#problib.evolution.candidate.AlphaString.random">random</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="problib.evolution.candidate.BitString" href="#problib.evolution.candidate.BitString">BitString</a></code></h4>
</li>
<li>
<h4><code><a title="problib.evolution.candidate.Candidate" href="#problib.evolution.candidate.Candidate">Candidate</a></code></h4>
<ul class="">
<li><code><a title="problib.evolution.candidate.Candidate.epigenesis" href="#problib.evolution.candidate.Candidate.epigenesis">epigenesis</a></code></li>
<li><code><a title="problib.evolution.candidate.Candidate.random" href="#problib.evolution.candidate.Candidate.random">random</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="problib.evolution.candidate.NeuralNetwork" href="#problib.evolution.candidate.NeuralNetwork">NeuralNetwork</a></code></h4>
<ul class="">
<li><code><a title="problib.evolution.candidate.NeuralNetwork.epigenesis" href="#problib.evolution.candidate.NeuralNetwork.epigenesis">epigenesis</a></code></li>
<li><code><a title="problib.evolution.candidate.NeuralNetwork.predict" href="#problib.evolution.candidate.NeuralNetwork.predict">predict</a></code></li>
<li><code><a title="problib.evolution.candidate.NeuralNetwork.random" href="#problib.evolution.candidate.NeuralNetwork.random">random</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>